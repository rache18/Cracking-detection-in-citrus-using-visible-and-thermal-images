{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOyYzep4Yesa4OLK4hbJuEi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"GC5kQDTblCUW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727605536079,"user_tz":-180,"elapsed":44748,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}},"outputId":"294f87f0-fba6-4f41-c886-4dad801f3866"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j9O-ZcdkbB0j","executionInfo":{"status":"ok","timestamp":1727605572101,"user_tz":-180,"elapsed":696,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}},"outputId":"86eac872-0e2c-48f1-c9d5-d92a0352c9bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Sep 29 10:26:11 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   50C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","source":["import os\n","HOME = os.getcwd()\n","print(\"HOME:\", HOME)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K76VM0cmbWAL","executionInfo":{"status":"ok","timestamp":1727605576929,"user_tz":-180,"elapsed":682,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}},"outputId":"fb380be0-2f61-4033-ce1f-835fafee34d3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["HOME: /content\n"]}]},{"cell_type":"markdown","source":["## Install Segment Anything Model (SAM) and other dependencies"],"metadata":{"id":"bR1FeGfPbfgf"}},{"cell_type":"code","source":["!pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HnUYQiaHbacA","executionInfo":{"status":"ok","timestamp":1727605587466,"user_tz":-180,"elapsed":8868,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}},"outputId":"84db4c52-6a4b-4026-c730-7a137392d42b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["!pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yH7btw2Fbnr2","executionInfo":{"status":"ok","timestamp":1727605594422,"user_tz":-180,"elapsed":6962,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}},"outputId":"31f9f683-8fb3-49d2-8762-ac29ad87b892"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/80.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/66.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/151.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.5/151.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/213.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.7/213.7 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.4/727.4 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["### Download SAM weights"],"metadata":{"id":"ggFj3DlVbsPI"}},{"cell_type":"code","source":["!mkdir -p {HOME}/weights\n","!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P {HOME}/weights"],"metadata":{"id":"22fobvCAbrRU","executionInfo":{"status":"ok","timestamp":1727605661683,"user_tz":-180,"elapsed":20607,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n","print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xJftrbSGb_C4","executionInfo":{"status":"ok","timestamp":1727605702144,"user_tz":-180,"elapsed":679,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}},"outputId":"7b11d926-6319-4923-c9e4-e0ba12cc063b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/weights/sam_vit_h_4b8939.pth ; exist: True\n"]}]},{"cell_type":"markdown","source":["## Load Model"],"metadata":{"id":"Wb5ds_4YcCAv"}},{"cell_type":"code","source":["import torch\n","\n","DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","MODEL_TYPE = \"vit_h\""],"metadata":{"id":"jxndstsdcEJZ","executionInfo":{"status":"ok","timestamp":1727605712134,"user_tz":-180,"elapsed":6703,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n","\n","sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZNPOVySYcJup","executionInfo":{"status":"ok","timestamp":1727605737644,"user_tz":-180,"elapsed":20349,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}},"outputId":"96a64bc0-75af-4dfe-ddc4-4f7e42ce2c5b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(f)\n"]}]},{"cell_type":"markdown","source":["### Utils Supporting Dataset Processing\n","\n","A couple of helper functions that, unfortunately, we have to write ourselves to facilitate the processing of COCO annotations."],"metadata":{"id":"10ymqpvWfsBi"}},{"cell_type":"code","source":["import numpy as np\n","from dataclasses import dataclass\n","from typing import List, Tuple, Union, Optional\n","from dataclasses_json import dataclass_json\n","from supervision import Detections\n","\n","\n","@dataclass_json\n","@dataclass\n","class COCOCategory:\n","    id: int\n","    name: str\n","    supercategory: str\n","\n","\n","@dataclass_json\n","@dataclass\n","class COCOImage:\n","    id: int\n","    width: int\n","    height: int\n","    file_name: str\n","    license: int\n","    date_captured: str\n","    coco_url: Optional[str] = None\n","    flickr_url: Optional[str] = None\n","\n","\n","@dataclass_json\n","@dataclass\n","class COCOAnnotation:\n","    id: int\n","    image_id: int\n","    category_id: int\n","    segmentation: List[List[float]]\n","    area: float\n","    bbox: Tuple[float, float, float, float]\n","    iscrowd: int\n","\n","\n","@dataclass_json\n","@dataclass\n","class COCOLicense:\n","    id: int\n","    name: str\n","    url: str\n","\n","\n","@dataclass_json\n","@dataclass\n","class COCOJson:\n","    images: List[COCOImage]\n","    annotations: List[COCOAnnotation]\n","    categories: List[COCOCategory]\n","    licenses: List[COCOLicense]\n","\n","\n","def load_coco_json(json_file: str) -> COCOJson:\n","    import json\n","\n","    with open(json_file, \"r\") as f:\n","        json_data = json.load(f)\n","\n","    return COCOJson.from_dict(json_data)\n","\n","\n","class COCOJsonUtility:\n","    @staticmethod\n","    def get_annotations_by_image_id(coco_data: COCOJson, image_id: int) -> List[COCOAnnotation]:\n","        return [annotation for annotation in coco_data.annotations if annotation.image_id == image_id]\n","\n","    @staticmethod\n","    def get_annotations_by_image_path(coco_data: COCOJson, image_path: str) -> Optional[List[COCOAnnotation]]:\n","        image = COCOJsonUtility.get_image_by_path(coco_data, image_path)\n","        if image:\n","            return COCOJsonUtility.get_annotations_by_image_id(coco_data, image.id)\n","        else:\n","            return None\n","\n","    @staticmethod\n","    def get_image_by_path(coco_data: COCOJson, image_path: str) -> Optional[COCOImage]:\n","        for image in coco_data.images:\n","            if image.file_name == image_path:\n","                return image\n","        return None\n","\n","    @staticmethod\n","    def annotations2detections(annotations: List[COCOAnnotation]) -> Detections:\n","        class_id, xyxy = [], []\n","\n","        for annotation in annotations:\n","            x_min, y_min, width, height = annotation.bbox\n","            class_id.append(annotation.category_id)\n","            xyxy.append([\n","                x_min,\n","                y_min,\n","                x_min + width,\n","                y_min + height\n","            ])\n","\n","        return Detections(\n","            xyxy=np.array(xyxy, dtype=int),\n","            class_id=np.array(class_id, dtype=int)\n","        )"],"metadata":{"id":"fj1Ik71Qfs5H","executionInfo":{"status":"ok","timestamp":1727605745183,"user_tz":-180,"elapsed":1646,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### Download Dataset from Roboflow - run all images (train valid and test ) for both rgb and thermal converted min max version"],"metadata":{"id":"C2zV0gC8cWEd"}},{"cell_type":"code","source":["mask_predictor = SamPredictor(sam)"],"metadata":{"id":"VptfU7vPgPhK","executionInfo":{"status":"ok","timestamp":1727606422661,"user_tz":-180,"elapsed":682,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from roboflow import Roboflow\n","import supervision as sv\n","import random\n","\n","def load_dataset(api_key, workspace, project_name, version_number):\n","    rf = Roboflow(api_key=api_key)\n","    project = rf.workspace(workspace).project(project_name)\n","    version = project.version(version_number)\n","    return version.download(\"coco\")\n","\n","def get_image_list_and_annotations(dataset_location, subdirectory):\n","    annotations_file_path = os.path.join(dataset_location, subdirectory, \"_annotations.coco.json\")\n","    coco_data = load_coco_json(json_file=annotations_file_path)\n","    image_list = [image.file_name for image in coco_data.images]\n","    return image_list, coco_data\n","\n","def process_images(image_list, dataset_location, subdirectory, coco_data, output_directory):\n","    df_segments = pd.DataFrame(columns=['Image Name', 'Area'])\n","    masked_directory = os.path.join(output_directory, subdirectory + \"_masked\")\n","    os.makedirs(masked_directory, exist_ok=True)\n","\n","    for image_name in image_list:\n","        image_path = os.path.join(dataset_location, subdirectory, image_name)\n","        image = cv2.imread(image_path)\n","        if image is None:\n","            continue\n","\n","        processed_image, area = process_single_image(image, coco_data, image_name)\n","        if processed_image is not None:\n","            save_path = os.path.join(masked_directory, f\"{image_name}_mask.png\")\n","            cv2.imwrite(save_path, processed_image)\n","            df_segments.loc[len(df_segments)] = [image_name, area]\n","\n","    return df_segments\n","\n","def process_single_image(image, coco_data, image_name):\n","    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    annotations = COCOJsonUtility.get_annotations_by_image_path(coco_data=coco_data, image_path=image_name)\n","    ground_truth = COCOJsonUtility.annotations2detections(annotations=annotations)\n","    ground_truth.class_id -= 1\n","\n","    mask_predictor.set_image(image_rgb)\n","    masks, scores, logits = mask_predictor.predict(box=ground_truth.xyxy[0], multimask_output=True)\n","\n","    if len(masks) > 0:\n","        mask = masks[0]\n","        binary_mask = np.invert((mask > 0).astype(np.uint8) * 255)\n","        area = np.sum(binary_mask == 255)\n","        return binary_mask, area\n","    return None, 0\n","\n","# Main execution\n","\n","#RGB\n","# api_key = \"aCFGMWNyipynQqvPzfaM\"\n","# workspace = \"citrus-a0lk2\"\n","# project_name = \"segment-performance-measure-2\"\n","# version_number = 2\n","\n","#THERMAL\n","api_key= \"aCFGMWNyipynQqvPzfaM\"\n","workspace = \"citrus-a0lk2\"\n","project_name = \"performance_measure_thermal\"\n","version_number = 1\n","\n","\n","dataset = load_dataset(api_key, workspace, project_name, version_number)\n","dataset_location = dataset.location\n","subdirectories = [\"train\", \"valid\", \"test\"]  # List of subdirectories\n","# output_directory = \"/content/drive/Othercomputers/My PC/Thesis/roboflowV6_1169/SAM\"\n","output_directory = \"/content/drive/Othercomputers/My PC/Thesis/roboflowV6_1169/SAM_MANUAL_THERMAL\"\n","\n","all_segments = pd.DataFrame()\n","\n","for subdirectory in subdirectories:\n","    images, coco_data = get_image_list_and_annotations(dataset_location, subdirectory)\n","    df_segments = process_images(images, dataset_location, subdirectory, coco_data, output_directory)\n","    all_segments = pd.concat([all_segments, df_segments], ignore_index=True)\n","\n","print(all_segments)\n"],"metadata":{"id":"LEYwfGD27CFf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727606663494,"user_tz":-180,"elapsed":234916,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}},"outputId":"3c935efa-ff30-4062-a1bf-c8b68f72a1f7"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["loading Roboflow workspace...\n","loading Roboflow project...\n","                                           Image Name    Area\n","0   FLIR1616_jpg.rf.23655aff9a1a3dea8d49ddeeda4ae9...  661068\n","1   FLIR1425_jpg.rf.0363c9af57273af40c8464bdcdbbbb...  713035\n","2   FLIR1958_jpg.rf.326eebd8b60020d28631966a3f60ae...  643093\n","3   FLIR1080_jpg.rf.3e06ec5b6b3458dc58f344db434618...  754822\n","4   FLIR1651_jpg.rf.20f8c6dd41dd0a33776ba64c31951b...  649038\n","..                                                ...     ...\n","94  FLIR1999_jpg.rf.ab9e89cf1c77cffa9921b5dbb4a4a9...  664859\n","95  FLIR1311_jpg.rf.074c87bc07f5a5a81fb1a87b439b3e...  703043\n","96  FLIR1914_jpg.rf.19fcbe0cce8f4c23ab0e17843f1759...  703159\n","97  FLIR1407_jpg.rf.a7a0ef9d66f7ef6a93cbc39ce99b0d...  682167\n","98  FLIR1851_jpg.rf.3f386375898e6498b0b24d897886d3...  690299\n","\n","[99 rows x 2 columns]\n"]}]},{"cell_type":"code","source":["%cd {HOME}\n","\n","import roboflow\n","from roboflow import Roboflow\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hv2Tpbe-c49q","executionInfo":{"status":"ok","timestamp":1726639540762,"user_tz":-180,"elapsed":341,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}},"outputId":"0676a8ac-0f48-442b-a5e4-69521eb93b51"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"aCFGMWNyipynQqvPzfaM\")\n","project = rf.workspace(\"citrus-a0lk2\").project(\"segment-performance-measure-2\")\n","version = project.version(2)\n","dataset = version.download(\"coco\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r16_SEKBcX-U","executionInfo":{"status":"ok","timestamp":1726639553287,"user_tz":-180,"elapsed":9760,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}},"outputId":"55be35d2-52db-4c52-843d-4930b29ff131"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading Roboflow workspace...\n","loading Roboflow project...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading Dataset Version Zip in Segment-performance-measure-2-2 to coco:: 100%|██████████| 229172/229172 [00:06<00:00, 34668.82it/s]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\n","Extracting Dataset Version Zip to Segment-performance-measure-2-2 in coco:: 100%|██████████| 108/108 [00:00<00:00, 139.79it/s]\n"]}]},{"cell_type":"code","source":["import os\n","\n","DATA_SET_SUBDIRECTORY = \"valid\"\n","ANNOTATIONS_FILE_NAME = \"_annotations.coco.json\"\n","IMAGES_DIRECTORY_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY)\n","ANNOTATIONS_FILE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, ANNOTATIONS_FILE_NAME)"],"metadata":{"id":"Qgqe1ad8cZvJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["coco_data = load_coco_json(json_file=ANNOTATIONS_FILE_PATH)\n","\n","CLASSES = [\n","    category.name\n","    for category\n","    in coco_data.categories\n","    if category.supercategory != 'none'\n","]\n","\n","IMAGES = [image.file_name for image in coco_data.images]\n"],"metadata":{"id":"mqwxJAREfYXT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CLASSES"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"os42TlXqf6ei","executionInfo":{"status":"ok","timestamp":1726639558949,"user_tz":-180,"elapsed":338,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}},"outputId":"6d3bb198-94f8-4dd1-d926-94680c6993db"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['mandarin']"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# set random seed to allow easy reproduction of the experiment\n","\n","import random\n","random.seed(10)"],"metadata":{"id":"iINZ0vxDf9-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import supervision as sv"],"metadata":{"id":"p9mWypb7gAlQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EXAMPLE_IMAGE_NAME = random.choice(IMAGES)\n","EXAMPLE_IMAGE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, EXAMPLE_IMAGE_NAME)\n","\n","# load dataset annotations\n","annotations = COCOJsonUtility.get_annotations_by_image_path(coco_data=coco_data, image_path=EXAMPLE_IMAGE_NAME)\n","ground_truth = COCOJsonUtility.annotations2detections(annotations=annotations)\n","\n","# small hack - coco numerate classes from 1, model from 0 + we drop first redundant class from coco json\n","ground_truth.class_id = ground_truth.class_id - 1\n","\n","# load image\n","image_bgr = cv2.imread(EXAMPLE_IMAGE_PATH)\n","image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n","\n","# initiate annotator\n","bounding_box_annotator = sv.BoundingBoxAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n","mask_annotator = sv.MaskAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n","\n","\n","# annotate ground truth\n","annotated_frame_ground_truth = bounding_box_annotator.annotate(scene=image_bgr.copy(), detections=ground_truth)\n","\n","# run SAM inference\n","mask_predictor.set_image(image_rgb)\n","\n","masks, scores, logits = mask_predictor.predict(\n","    box=ground_truth.xyxy[0],\n","    multimask_output=True\n",")\n","\n","detections = sv.Detections(\n","    xyxy=sv.mask_to_xyxy(masks=masks),\n","    mask=masks\n",")\n","detections = detections[detections.area == np.max(detections.area)]\n","\n","annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n","\n","sv.plot_images_grid(\n","    images=[annotated_frame_ground_truth, annotated_image],\n","    grid_size=(1, 2),\n","    titles=['source image', 'segmented image']\n",")\n","\n","\n"],"metadata":{"id":"uq8I_9q0gEsM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Total images:\", len(IMAGES))\n"],"metadata":{"id":"6khB48ybnBUr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from roboflow import Roboflow"],"metadata":{"id":"ofwSlQeSwlVi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rf = Roboflow(api_key=\"aCFGMWNyipynQqvPzfaM\")\n","project = rf.workspace(\"citrus-a0lk2\").project(\"segment-performance-measure-2\")\n","version = project.version(2)\n","dataset = version.download(\"coco\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9b6qQXTM_PYm","executionInfo":{"status":"ok","timestamp":1726639575645,"user_tz":-180,"elapsed":1397,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}},"outputId":"acfc62ca-9cca-4e25-ab70-21b5677b43ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading Roboflow workspace...\n","loading Roboflow project...\n"]}]},{"cell_type":"code","source":["# Define paths for 'valid' subdirectory\n","DATA_SET_SUBDIRECTORY = \"valid\"\n","ANNOTATIONS_FILE_NAME = \"_annotations.coco.json\"\n","IMAGES_DIRECTORY_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY)\n","ANNOTATIONS_FILE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, ANNOTATIONS_FILE_NAME)\n","\n","coco_data = load_coco_json(json_file=ANNOTATIONS_FILE_PATH)\n","\n","\n","CLASSES = [\n","    category.name\n","    for category\n","    in coco_data.categories\n","    if category.supercategory != 'none'\n","]\n","\n","IMAGES = [image.file_name for image in coco_data.images]"],"metadata":{"id":"4X9MCe4Etcdj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# DataFrame for storing image names and areas\n","df_segments_valid = pd.DataFrame(columns=['Image Name', 'Area'])\n","\n","import random\n","random.seed(10)\n","\n","import supervision as sv\n","\n","# Process each image\n","for EXAMPLE_IMAGE_NAME in IMAGES:\n","    EXAMPLE_IMAGE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, EXAMPLE_IMAGE_NAME)\n","    image_bgr = cv2.imread(EXAMPLE_IMAGE_PATH)\n","    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n","    annotations = COCOJsonUtility.get_annotations_by_image_path(coco_data=coco_data, image_path=EXAMPLE_IMAGE_NAME)\n","    ground_truth = COCOJsonUtility.annotations2detections(annotations=annotations)\n","    ground_truth.class_id = ground_truth.class_id - 1\n","\n","    # Annotate using the new BoxAnnotator\n","    bounding_box_annotator = sv.BoxAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n","    mask_annotator = sv.MaskAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n","    annotated_frame_ground_truth = bounding_box_annotator.annotate(scene=image_bgr.copy(), detections=ground_truth)\n","\n","    mask_predictor.set_image(image_rgb)\n","    masks, scores, logits = mask_predictor.predict(\n","        box=ground_truth.xyxy[0],\n","        multimask_output=True\n","    )\n","\n","    detections = sv.Detections(\n","        xyxy=sv.mask_to_xyxy(masks=masks),\n","        mask=masks\n","    )\n","    max_area = np.max([np.sum(mask) for mask in masks])\n","    detections = detections[detections.area == max_area]\n","\n","    annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n","    save_path = f\"/content/drive/Othercomputers/My PC/Thesis/roboflowV6_1169/SAM_2/{EXAMPLE_IMAGE_NAME}_annotated.png\"\n","    cv2.imwrite(save_path, annotated_image)\n","\n","    # Update the DataFrame with new data\n","    df_segments_valid.loc[len(df_segments_valid)] = [EXAMPLE_IMAGE_NAME, max_area]\n","\n","# Display the DataFrame\n","print(df_segments_valid)\n"],"metadata":{"id":"xEsCCcZ9kOgL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["run again save the binary mask valid"],"metadata":{"id":"mUU_z1sU11iE"}},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from roboflow import Roboflow\n","import supervision as sv\n","import random\n","\n","# Initialize Roboflow and download the dataset\n","rf = Roboflow(api_key=\"aCFGMWNyipynQqvPzfaM\")\n","project = rf.workspace(\"citrus-a0lk2\").project(\"segment-performance-measure-2\")\n","version = project.version(2)\n","dataset = version.download(\"coco\")\n","\n","# Define paths for 'valid' subdirectory\n","DATA_SET_SUBDIRECTORY = \"valid\"\n","ANNOTATIONS_FILE_NAME = \"_annotations.coco.json\"\n","IMAGES_DIRECTORY_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY)\n","ANNOTATIONS_FILE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, ANNOTATIONS_FILE_NAME)\n","\n","coco_data = load_coco_json(json_file=ANNOTATIONS_FILE_PATH)\n","\n","# DataFrame for storing image names and areas\n","df_segments_valid = pd.DataFrame(columns=['Image Name', 'Area'])\n","\n","random.seed(10)\n","\n","\n","# Path for the new 'masked' folder\n","masked_directory = \"/content/drive/Othercomputers/My PC/Thesis/roboflowV6_1169/SAM_2/masked\"\n","os.makedirs(masked_directory, exist_ok=True)  # Ensure the directory exists\n","\n","\n","# Process each image in the 'valid' directory\n","for EXAMPLE_IMAGE_NAME in [image.file_name for image in coco_data.images]:\n","    EXAMPLE_IMAGE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, EXAMPLE_IMAGE_NAME)\n","    image_bgr = cv2.imread(EXAMPLE_IMAGE_PATH)\n","    if image_bgr is None:\n","        continue  # Skip if the image cannot be loaded\n","\n","    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n","    annotations = COCOJsonUtility.get_annotations_by_image_path(coco_data=coco_data, image_path=EXAMPLE_IMAGE_NAME)\n","    ground_truth = COCOJsonUtility.annotations2detections(annotations=annotations)\n","    ground_truth.class_id -= 1\n","\n","    # Annotate using the BoxAnnotator\n","    bounding_box_annotator = sv.BoxAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n","    mask_annotator = sv.MaskAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n","    annotated_frame_ground_truth = bounding_box_annotator.annotate(scene=image_bgr.copy(), detections=ground_truth)\n","\n","    mask_predictor.set_image(image_rgb)\n","    masks, scores, logits = mask_predictor.predict(\n","        box=ground_truth.xyxy[0],\n","        multimask_output=True\n","    )\n","\n","    # Save only the first mask if masks are available\n","    if len(masks) > 0:\n","        first_mask = masks[0]  # Take the first mask\n","        binary_mask = np.invert((first_mask > 0).astype(np.uint8) * 255)\n","        binary_mask_save_path = os.path.join(masked_directory, f\"{EXAMPLE_IMAGE_NAME}_mask.png\")\n","        cv2.imwrite(binary_mask_save_path, binary_mask)\n","\n","    max_area = np.max([np.sum(first_mask) for first_mask in [masks[0]]] if len(masks) > 0 else [0])\n","    df_segments_train.loc[len(df_segments_train)] = [EXAMPLE_IMAGE_NAME, max_area]\n","\n","# Display the DataFrame\n","print(df_segments_train)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vl0VjgOv15XI","executionInfo":{"status":"ok","timestamp":1726641416841,"user_tz":-180,"elapsed":65690,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}},"outputId":"5fe0612f-19e0-4f4b-defc-182164afb358"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading Roboflow workspace...\n","loading Roboflow project...\n","                                           Image Name     Area\n","0   1D3A5563_JPG.rf.18495fb53856b511f81300816c2ce8...  6446404\n","1   1D3A3200_JPG.rf.29aeb7444ee567536ac2bc7b56d457...  1774001\n","2   1D3A3460_JPG.rf.2e7d2b8cf9f4bdf35a3cdd6ce03858...  8937229\n","3   1D3A5385_JPG.rf.2fe3c57d1c45685b6ad32e1464a6b3...  6082396\n","4   1D3A5496_JPG.rf.0ca7bf0b4bee5c54f37d601c10ea46...  5152810\n","..                                                ...      ...\n","95  1D3A4273_JPG.rf.9fa46b0866d5b88ffa0cadfcd63123...  6208076\n","96  1D3A5458_JPG.rf.93b91aedf33530af08a4f59b4d2c69...  8407367\n","97  1D3A5619_JPG.rf.f549bbbce325984fa7d7412fa636af...  4880941\n","98  1D3A5091_JPG.rf.d108d32f9115ecb2529bec341a12d2...  6191915\n","99  1D3A4408_JPG.rf.98dc9188aa1f6681359695533a42a7...  4812010\n","\n","[100 rows x 2 columns]\n"]}]},{"cell_type":"code","source":["# Define paths for 'test' subdirectory\n","DATA_SET_SUBDIRECTORY = \"test\"\n","ANNOTATIONS_FILE_NAME = \"_annotations.coco.json\"\n","IMAGES_DIRECTORY_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY)\n","ANNOTATIONS_FILE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, ANNOTATIONS_FILE_NAME)\n","\n","coco_data = load_coco_json(json_file=ANNOTATIONS_FILE_PATH)\n","\n","\n","CLASSES = [\n","    category.name\n","    for category\n","    in coco_data.categories\n","    if category.supercategory != 'none'\n","]\n","\n","IMAGES = [image.file_name for image in coco_data.images]"],"metadata":{"id":"yxrOr_INtD4O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the directory for saving binary masks\n","mask_save_directory = \"/content/drive/Othercomputers/My PC/Thesis/roboflowV6_1169/SAM_2/maskedtest\"\n","os.makedirs(mask_save_directory, exist_ok=True)  # Ensure the directory exists\n","\n","# Assuming 'masks' is a list of NumPy arrays where each element is a binary mask\n","# Let's say 'masks' is obtained from your mask predictor\n","for idx, EXAMPLE_IMAGE_NAME in enumerate(IMAGES):\n","    EXAMPLE_IMAGE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, EXAMPLE_IMAGE_NAME)\n","    image_bgr = cv2.imread(EXAMPLE_IMAGE_PATH)\n","    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n","\n","    # Assuming mask_predictor is set up as shown in your previous messages\n","    masks, scores, logits = mask_predictor.predict(\n","        box=ground_truth.xyxy[0],  # Ensure this is correctly set up\n","        multimask_output=True\n","    )\n","\n","    # Process each mask\n","    for mask_idx, mask in enumerate(masks):\n","        # Convert mask to binary format\n","        binary_mask = (mask > 0).astype(np.uint8) * 255  # Ensure mask is binary {0, 255}\n","\n","        # Save the binary mask to file\n","        mask_filename = f\"{EXAMPLE_IMAGE_NAME}_mask_{mask_idx}.png\"\n","        mask_path = os.path.join(mask_save_directory, mask_filename)\n","        cv2.imwrite(mask_path, binary_mask)\n","\n","print(\"All binary masks have been generated and saved.\")\n"],"metadata":{"id":"McX-MCKcnhPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Initialize Roboflow and download the dataset\n","rf = Roboflow(api_key=\"aCFGMWNyipynQqvPzfaM\")\n","project = rf.workspace(\"citrus-a0lk2\").project(\"segment-performance-measure-2\")\n","version = project.version(2)\n","dataset = version.download(\"coco\")\n","\n","# Define paths for 'test' subdirectory\n","DATA_SET_SUBDIRECTORY = \"test\"\n","ANNOTATIONS_FILE_NAME = \"_annotations.coco.json\"\n","IMAGES_DIRECTORY_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY)\n","ANNOTATIONS_FILE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, ANNOTATIONS_FILE_NAME)\n","\n","coco_data = load_coco_json(json_file=ANNOTATIONS_FILE_PATH)\n","\n","# DataFrame for storing image names and areas\n","df_segments_test = pd.DataFrame(columns=['Image Name', 'Area'])\n","\n","random.seed(10)\n","\n","# Process each image in the 'test' directory\n","for EXAMPLE_IMAGE_NAME in [image.file_name for image in coco_data.images]:\n","    EXAMPLE_IMAGE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, EXAMPLE_IMAGE_NAME)\n","    image_bgr = cv2.imread(EXAMPLE_IMAGE_PATH)\n","    if image_bgr is None:\n","        continue  # Skip if the image cannot be loaded\n","\n","    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n","    annotations = COCOJsonUtility.get_annotations_by_image_path(coco_data=coco_data, image_path=EXAMPLE_IMAGE_NAME)\n","    ground_truth = COCOJsonUtility.annotations2detections(annotations=annotations)\n","    ground_truth.class_id = ground_truth.class_id - 1\n","\n","    # Annotate using the BoxAnnotator\n","    bounding_box_annotator = sv.BoxAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n","    mask_annotator = sv.MaskAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n","    annotated_frame_ground_truth = bounding_box_annotator.annotate(scene=image_bgr.copy(), detections=ground_truth)\n","\n","    mask_predictor.set_image(image_rgb)\n","    masks, scores, logits = mask_predictor.predict(\n","        box=ground_truth.xyxy[0],\n","        multimask_output=True\n","    )\n","\n","    max_area = np.max([np.sum(mask) for mask in masks])\n","    detections = sv.Detections(\n","        xyxy=sv.mask_to_xyxy(masks=masks),\n","        mask=masks\n","    )\n","    detections = detections[detections.area == max_area]\n","\n","    annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n","    save_path = f\"/content/drive/Othercomputers/My PC/Thesis/roboflowV6_1169/SAM_2/{DATA_SET_SUBDIRECTORY}_{EXAMPLE_IMAGE_NAME}_annotated.png\"\n","    cv2.imwrite(save_path, annotated_image)\n","\n","    # Update the DataFrame with new data\n","    df_segments_test.loc[len(df_segments_test)] = [EXAMPLE_IMAGE_NAME, max_area]\n","\n","# Display the DataFrame\n","print(df_segments_test)\n"],"metadata":{"id":"yD-ib1K0AoKu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["run again but save the binary mask - test"],"metadata":{"id":"Ek_XUXcQ0lsk"}},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from roboflow import Roboflow\n","import supervision as sv\n","import random\n","\n","# Initialize Roboflow and download the dataset\n","rf = Roboflow(api_key=\"aCFGMWNyipynQqvPzfaM\")\n","project = rf.workspace(\"citrus-a0lk2\").project(\"segment-performance-measure-2\")\n","version = project.version(2)\n","dataset = version.download(\"coco\")\n","\n","# Define paths for 'test' subdirectory\n","DATA_SET_SUBDIRECTORY = \"test\"\n","ANNOTATIONS_FILE_NAME = \"_annotations.coco.json\"\n","IMAGES_DIRECTORY_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY)\n","ANNOTATIONS_FILE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, ANNOTATIONS_FILE_NAME)\n","\n","coco_data = load_coco_json(json_file=ANNOTATIONS_FILE_PATH)\n","\n","# DataFrame for storing image names and areas\n","df_segments_test = pd.DataFrame(columns=['Image Name', 'Area'])\n","\n","random.seed(10)\n","\n","\n","# Path for the new 'masked' folder\n","masked_directory = \"/content/drive/Othercomputers/My PC/Thesis/roboflowV6_1169/SAM_2/masked\"\n","os.makedirs(masked_directory, exist_ok=True)  # Ensure the directory exists\n","\n","\n","# Process each image in the 'test' directory\n","for EXAMPLE_IMAGE_NAME in [image.file_name for image in coco_data.images]:\n","    EXAMPLE_IMAGE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, EXAMPLE_IMAGE_NAME)\n","    image_bgr = cv2.imread(EXAMPLE_IMAGE_PATH)\n","    if image_bgr is None:\n","        continue  # Skip if the image cannot be loaded\n","\n","    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n","    annotations = COCOJsonUtility.get_annotations_by_image_path(coco_data=coco_data, image_path=EXAMPLE_IMAGE_NAME)\n","    ground_truth = COCOJsonUtility.annotations2detections(annotations=annotations)\n","    ground_truth.class_id -= 1\n","\n","    # Annotate using the BoxAnnotator\n","    bounding_box_annotator = sv.BoxAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n","    mask_annotator = sv.MaskAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n","    annotated_frame_ground_truth = bounding_box_annotator.annotate(scene=image_bgr.copy(), detections=ground_truth)\n","\n","    mask_predictor.set_image(image_rgb)\n","    masks, scores, logits = mask_predictor.predict(\n","        box=ground_truth.xyxy[0],\n","        multimask_output=True\n","    )\n","\n","    # Save only the first mask if masks are available\n","    if len(masks) > 0:\n","        first_mask = masks[0]  # Take the first mask\n","        binary_mask = np.invert((first_mask > 0).astype(np.uint8) * 255)\n","        binary_mask_save_path = os.path.join(masked_directory, f\"{EXAMPLE_IMAGE_NAME}_mask.png\")\n","        cv2.imwrite(binary_mask_save_path, binary_mask)\n","\n","    max_area = np.max([np.sum(first_mask) for first_mask in [masks[0]]] if len(masks) > 0 else [0])\n","    df_segments_train.loc[len(df_segments_train)] = [EXAMPLE_IMAGE_NAME, max_area]\n","\n","# Display the DataFrame\n","print(df_segments_train)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gsiWCn_y0ksp","executionInfo":{"status":"ok","timestamp":1726641140436,"user_tz":-180,"elapsed":28926,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}},"outputId":"0aafdd03-3c44-47c5-e48d-d6d83b5a6a45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading Roboflow workspace...\n","loading Roboflow project...\n","                                           Image Name      Area\n","0   1D3A5563_JPG.rf.18495fb53856b511f81300816c2ce8...   6446404\n","1   1D3A3200_JPG.rf.29aeb7444ee567536ac2bc7b56d457...   1774001\n","2   1D3A3460_JPG.rf.2e7d2b8cf9f4bdf35a3cdd6ce03858...   8937229\n","3   1D3A5385_JPG.rf.2fe3c57d1c45685b6ad32e1464a6b3...   6082396\n","4   1D3A5496_JPG.rf.0ca7bf0b4bee5c54f37d601c10ea46...   5152810\n","..                                                ...       ...\n","74  1D3A4538_JPG.rf.8d2a5b50bd06d18a974472115344d4...   6567573\n","75  1D3A4875_JPG.rf.dc3823d411c57cc86fdd9c1453f8b3...  10271194\n","76  1D3A4312_JPG.rf.f88435080170bc2a89f8f807e184cb...   4492194\n","77  1D3A4884_JPG.rf.1a4655fd29878dfe6a90508e7b5f2c...   4881794\n","78  1D3A3277_JPG.rf.d3705c2db4d238fe28f509670ab1da...   6061895\n","\n","[79 rows x 2 columns]\n"]}]},{"cell_type":"code","source":["# Define paths for 'train' subdirectory\n","DATA_SET_SUBDIRECTORY = \"train\"\n","ANNOTATIONS_FILE_NAME = \"_annotations.coco.json\"\n","IMAGES_DIRECTORY_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY)\n","ANNOTATIONS_FILE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, ANNOTATIONS_FILE_NAME)\n","\n","coco_data = load_coco_json(json_file=ANNOTATIONS_FILE_PATH)\n","\n","\n","CLASSES = [\n","    category.name\n","    for category\n","    in coco_data.categories\n","    if category.supercategory != 'none'\n","]\n","\n","IMAGES = [image.file_name for image in coco_data.images]"],"metadata":{"id":"lHj0Wiy3tnH6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from roboflow import Roboflow\n","import supervision as sv\n","import random\n","\n","# Initialize Roboflow and download the dataset\n","rf = Roboflow(api_key=\"aCFGMWNyipynQqvPzfaM\")\n","project = rf.workspace(\"citrus-a0lk2\").project(\"segment-performance-measure-2\")\n","version = project.version(2)\n","dataset = version.download(\"coco\")\n","\n","# Define paths for 'train' subdirectory\n","DATA_SET_SUBDIRECTORY = \"train\"\n","ANNOTATIONS_FILE_NAME = \"_annotations.coco.json\"\n","IMAGES_DIRECTORY_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY)\n","ANNOTATIONS_FILE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, ANNOTATIONS_FILE_NAME)\n","\n","coco_data = load_coco_json(json_file=ANNOTATIONS_FILE_PATH)\n","\n","# DataFrame for storing image names and areas\n","df_segments_train = pd.DataFrame(columns=['Image Name', 'Area'])\n","\n","random.seed(10)\n","\n","# Process each image in the 'train' directory\n","for EXAMPLE_IMAGE_NAME in [image.file_name for image in coco_data.images]:\n","    EXAMPLE_IMAGE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, EXAMPLE_IMAGE_NAME)\n","    image_bgr = cv2.imread(EXAMPLE_IMAGE_PATH)\n","    if image_bgr is None:\n","        continue  # Skip if the image cannot be loaded\n","\n","    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n","    annotations = COCOJsonUtility.get_annotations_by_image_path(coco_data=coco_data, image_path=EXAMPLE_IMAGE_NAME)\n","    ground_truth = COCOJsonUtility.annotations2detections(annotations=annotations)\n","    ground_truth.class_id = ground_truth.class_id - 1\n","\n","    # Annotate using the BoxAnnotator\n","    bounding_box_annotator = sv.BoxAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n","    mask_annotator = sv.MaskAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n","    annotated_frame_ground_truth = bounding_box_annotator.annotate(scene=image_bgr.copy(), detections=ground_truth)\n","\n","    mask_predictor.set_image(image_rgb)\n","    masks, scores, logits = mask_predictor.predict(\n","        box=ground_truth.xyxy[0],\n","        multimask_output=True\n","    )\n","\n","    max_area = np.max([np.sum(mask) for mask in masks])\n","    detections = sv.Detections(\n","        xyxy=sv.mask_to_xyxy(masks=masks),\n","        mask=masks\n","    )\n","    detections = detections[detections.area == max_area]\n","\n","    annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n","    save_path = f\"/content/drive/Othercomputers/My PC/Thesis/roboflowV6_1169/SAM_2/{DATA_SET_SUBDIRECTORY}_{EXAMPLE_IMAGE_NAME}_annotated.png\"\n","    cv2.imwrite(save_path, annotated_image)\n","\n","    # Update the DataFrame with new data\n","    df_segments_train.loc[len(df_segments_train)] = [EXAMPLE_IMAGE_NAME, max_area]\n","\n","# Display the DataFrame\n","print(df_segments_train)\n"],"metadata":{"id":"mBeJdqi7BFc3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["run again but save the binary mask - train"],"metadata":{"id":"WiyuXwNO0csg"}},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from roboflow import Roboflow\n","import supervision as sv\n","import random\n","\n","# Initialize Roboflow and download the dataset\n","rf = Roboflow(api_key=\"aCFGMWNyipynQqvPzfaM\")\n","project = rf.workspace(\"citrus-a0lk2\").project(\"segment-performance-measure-2\")\n","version = project.version(2)\n","dataset = version.download(\"coco\")\n","\n","# Define paths for 'train' subdirectory\n","DATA_SET_SUBDIRECTORY = \"train\"\n","ANNOTATIONS_FILE_NAME = \"_annotations.coco.json\"\n","IMAGES_DIRECTORY_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY)\n","ANNOTATIONS_FILE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, ANNOTATIONS_FILE_NAME)\n","\n","coco_data = load_coco_json(json_file=ANNOTATIONS_FILE_PATH)\n","\n","# DataFrame for storing image names and areas\n","df_segments_train = pd.DataFrame(columns=['Image Name', 'Area'])\n","\n","random.seed(10)\n","\n","\n","# Path for the new 'masked' folder\n","masked_directory = \"/content/drive/Othercomputers/My PC/Thesis/roboflowV6_1169/SAM_2/masked\"\n","os.makedirs(masked_directory, exist_ok=True)  # Ensure the directory exists\n","\n","\n","# Process each image in the 'train' directory\n","for EXAMPLE_IMAGE_NAME in [image.file_name for image in coco_data.images]:\n","    EXAMPLE_IMAGE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, EXAMPLE_IMAGE_NAME)\n","    image_bgr = cv2.imread(EXAMPLE_IMAGE_PATH)\n","    if image_bgr is None:\n","        continue  # Skip if the image cannot be loaded\n","\n","    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n","    annotations = COCOJsonUtility.get_annotations_by_image_path(coco_data=coco_data, image_path=EXAMPLE_IMAGE_NAME)\n","    ground_truth = COCOJsonUtility.annotations2detections(annotations=annotations)\n","    ground_truth.class_id -= 1\n","\n","    # Annotate using the BoxAnnotator\n","    bounding_box_annotator = sv.BoxAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n","    mask_annotator = sv.MaskAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n","    annotated_frame_ground_truth = bounding_box_annotator.annotate(scene=image_bgr.copy(), detections=ground_truth)\n","\n","    mask_predictor.set_image(image_rgb)\n","    masks, scores, logits = mask_predictor.predict(\n","        box=ground_truth.xyxy[0],\n","        multimask_output=True\n","    )\n","\n","    # Save only the first mask if masks are available\n","    if len(masks) > 0:\n","        first_mask = masks[0]  # Take the first mask\n","        binary_mask = np.invert((first_mask > 0).astype(np.uint8) * 255)\n","        binary_mask_save_path = os.path.join(masked_directory, f\"{EXAMPLE_IMAGE_NAME}_mask.png\")\n","        cv2.imwrite(binary_mask_save_path, binary_mask)\n","\n","    max_area = np.max([np.sum(first_mask) for first_mask in [masks[0]]] if len(masks) > 0 else [0])\n","    df_segments_train.loc[len(df_segments_train)] = [EXAMPLE_IMAGE_NAME, max_area]\n","\n","# Display the DataFrame\n","print(df_segments_train)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_VsWCnkusbM","executionInfo":{"status":"ok","timestamp":1726641003354,"user_tz":-180,"elapsed":223512,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}},"outputId":"249f6ea9-93f4-4e67-bb16-e4c3f22bf488"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                           Image Name     Area\n","0   1D3A5563_JPG.rf.18495fb53856b511f81300816c2ce8...  6446404\n","1   1D3A3200_JPG.rf.29aeb7444ee567536ac2bc7b56d457...  1774001\n","2   1D3A3460_JPG.rf.2e7d2b8cf9f4bdf35a3cdd6ce03858...  8937229\n","3   1D3A5385_JPG.rf.2fe3c57d1c45685b6ad32e1464a6b3...  6082396\n","4   1D3A5496_JPG.rf.0ca7bf0b4bee5c54f37d601c10ea46...  5152810\n","..                                                ...      ...\n","65  1D3A5378_JPG.rf.9dbdcc6b914762c8a038d4c3b3d0a3...  9830132\n","66  1D3A4555_JPG.rf.f7dcbca67442d97a55488e4b6c620c...  5791913\n","67  1D3A5388_JPG.rf.f79ad7acf3361070aa72d0ef778702...  4540062\n","68  1D3A4481_JPG.rf.fccd6779d03ee02a4fb07e7cc0415a...  8888796\n","69  1D3A4323_JPG.rf.e6029d9f5d2d9be719cb9bf57e9c01...  4636828\n","\n","[70 rows x 2 columns]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Assuming df_segments_valid, df_segments_test, df_segments_train are already defined\n","# Combine all three DataFrames into one\n","df_segments_combined = pd.concat([df_segments_train, df_segments_valid, df_segments_test], ignore_index=True)\n","\n","# Display the combined DataFrame\n","print(df_segments_combined)\n"],"metadata":{"id":"oxwzKa9WCN-r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","\n","# Assuming df_segments is your DataFrame\n","save_path = \"/content/drive/Othercomputers/My PC/Thesis/roboflowV6_1169/SAM_2/mandarins_data.xlsx\"\n","\n","# Create the directory if it does not exist\n","os.makedirs(os.path.dirname(save_path), exist_ok=True)\n","\n","# Save the DataFrame to an Excel file\n","df_segments_combined.to_excel(save_path, index=False)\n","\n","print(f\"DataFrame saved to {save_path}\")\n"],"metadata":{"id":"jQ_35uAO7ZGH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","\n","# Load the Excel file\n","df = pd.read_excel(\"/content/drive/Othercomputers/My PC/Thesis/roboflowV6_1169/SAM_2/mandarins_data.xlsx\")\n","\n","# Add or update the 'Unet area' column\n","df['Unet area'] = 0\n","\n","# Iterate through each row in the DataFrame\n","for index, row in df.iterrows():\n","    # Extract the first 8 characters from the image name to form the path to the UNet predicted image\n","    image_prefix = row['Image Name'][:8]\n","    unet_image_path = f\"/content/drive/Othercomputers/My PC/Thesis/roboflowV6_1169/unet/prediction/{image_prefix}.JPG\"\n","\n","    # Load the image in grayscale\n","    image = cv2.imread(unet_image_path, cv2.IMREAD_GRAYSCALE)\n","\n","    # Check if the image was loaded correctly\n","    if image is not None:\n","        # Count pixels that are not white (value not equal to 255)\n","        non_white_pixels = np.count_nonzero(image != 255)\n","        df.at[index, 'Unet area'] = non_white_pixels\n","    else:\n","        print(f\"Failed to load image at {unet_image_path}\")\n","\n","# Save the updated DataFrame back to Excel\n","df.to_excel(\"/content/drive/Othercomputers/My PC/Thesis/roboflowV6_1169/SAM_2/mandarins_data.xlsx\", index=False)\n","\n","# Print the DataFrame to verify the output\n","print(df.head())\n"],"metadata":{"id":"tnRlSjlB56uE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["try to make the code more effecient"],"metadata":{"id":"DNxrg6Rp4-Ml"}},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from roboflow import Roboflow\n","import supervision as sv\n","import random\n","\n","def load_dataset(api_key, workspace, project_name, version_number):\n","    rf = Roboflow(api_key=api_key)\n","    project = rf.workspace(workspace).project(project_name)\n","    version = project.version(version_number)\n","    return version.download(\"coco\")\n","\n","def get_image_list_and_annotations(dataset_location, subdirectory):\n","    annotations_file_path = os.path.join(dataset_location, subdirectory, \"_annotations.coco.json\")\n","    coco_data = load_coco_json(json_file=annotations_file_path)\n","    image_list = [image.file_name for image in coco_data.images]\n","    return image_list, coco_data\n","\n","def process_images(image_list, dataset_location, subdirectory, coco_data, output_directory):\n","    df_segments = pd.DataFrame(columns=['Image Name', 'Area'])\n","    masked_directory = os.path.join(output_directory, subdirectory + \"_masked\")\n","    os.makedirs(masked_directory, exist_ok=True)\n","\n","    for image_name in image_list:\n","        image_path = os.path.join(dataset_location, subdirectory, image_name)\n","        image = cv2.imread(image_path)\n","        if image is None:\n","            continue\n","\n","        processed_image, area = process_single_image(image, coco_data, image_name)\n","        if processed_image is not None:\n","            save_path = os.path.join(masked_directory, f\"{image_name}_mask.png\")\n","            cv2.imwrite(save_path, processed_image)\n","            df_segments.loc[len(df_segments)] = [image_name, area]\n","\n","    return df_segments\n","\n","def process_single_image(image, coco_data, image_name):\n","    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    annotations = COCOJsonUtility.get_annotations_by_image_path(coco_data=coco_data, image_path=image_name)\n","    ground_truth = COCOJsonUtility.annotations2detections(annotations=annotations)\n","    ground_truth.class_id -= 1\n","\n","    mask_predictor.set_image(image_rgb)\n","    masks, scores, logits = mask_predictor.predict(box=ground_truth.xyxy[0], multimask_output=True)\n","\n","    if len(masks) > 0:\n","        mask = masks[0]\n","        binary_mask = np.invert((mask > 0).astype(np.uint8) * 255)\n","        area = np.sum(binary_mask == 255)\n","        return binary_mask, area\n","    return None, 0\n","\n","# Main execution\n","api_key = \"aCFGMWNyipynQqvPzfaM\"\n","workspace = \"citrus-a0lk2\"\n","project_name = \"segment-performance-measure-2\"\n","version_number = 2\n","dataset = load_dataset(api_key, workspace, project_name, version_number)\n","dataset_location = dataset.location\n","subdirectories = [\"train\", \"valid\", \"test\"]  # List of subdirectories\n","output_directory = \"/content/drive/Othercomputers/My PC/Thesis/roboflowV6_1169/SAM\"\n","\n","all_segments = pd.DataFrame()\n","\n","for subdirectory in subdirectories:\n","    images, coco_data = get_image_list_and_annotations(dataset_location, subdirectory)\n","    df_segments = process_images(images, dataset_location, subdirectory, coco_data, output_directory)\n","    all_segments = pd.concat([all_segments, df_segments], ignore_index=True)\n","\n","print(all_segments)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"byT2cLdb5Qkd","executionInfo":{"status":"error","timestamp":1726642239859,"user_tz":-180,"elapsed":40552,"user":{"displayName":"Racheli Eliyahu","userId":"04395325802416145775"}},"outputId":"a287dd1c-3487-428a-f1db-eca083553ae3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading Roboflow workspace...\n","loading Roboflow project...\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-afd4157a300e>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubdirectories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoco_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_list_and_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mdf_segments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoco_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mall_segments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_segments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_segments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-afd4157a300e>\u001b[0m in \u001b[0;36mprocess_images\u001b[0;34m(image_list, dataset_location, subdirectory, coco_data, output_directory)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mprocessed_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_single_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoco_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprocessed_image\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{image_name}_mask.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-afd4157a300e>\u001b[0m in \u001b[0;36mprocess_single_image\u001b[0;34m(image, coco_data, image_name)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mmask_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_rgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxyxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultimask_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/segment_anything/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, point_coords, point_labels, box, mask_input, multimask_output, return_logits)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbox\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mbox_torch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mbox_torch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox_torch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask_input\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}